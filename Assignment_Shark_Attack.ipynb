{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qZMXOF2HeaNzOgS0Q_R7qr8VoCRe8Xc6",
      "authorship_tag": "ABX9TyO1V4yQ65x/TgN0wkthfQ1z",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MevrouwHelderder/Assignments/blob/main/Assignment_Shark_Attack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "metadata": {
        "id": "-GTLoS1qovCO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the essentials\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# importing the dataframe\n",
        "path = \"/content/drive/MyDrive/attacks.csv\"\n",
        "attacks = pd.read_csv(path, encoding=\"ISO-8859-1\")\n",
        "# Dropping columns.\n",
        "attacks_drop_columns = attacks.drop(\n",
        "    columns=[\n",
        "        \"Date\",\n",
        "        \"Year\",\n",
        "        \"Country\",\n",
        "        \"Area\",\n",
        "        \"Location\",\n",
        "        \"Name\",\n",
        "        \"Sex \",\n",
        "        \"Time\",\n",
        "        \"Investigator or Source\",\n",
        "        \"pdf\",\n",
        "        \"href formula\",\n",
        "        \"href\",\n",
        "        \"Case Number.1\",\n",
        "        \"Case Number.2\",\n",
        "        \"original order\",\n",
        "        \"Unnamed: 22\",\n",
        "        \"Unnamed: 23\",\n",
        "    ]\n",
        ")\n",
        "# Renaming columns.\n",
        "attacks_renamed = attacks_drop_columns.rename(\n",
        "    columns={\"Case Number\": \"Case\", \"Fatal (Y/N)\": \"Outcome\", \"Species \": \"Species\", \"Ages\": \"Age\"}\n",
        ")\n",
        "# Dropping rows.\n",
        "# Making a copy to prevent view vs copy issues later on.\n",
        "relevant_columns = list(attacks_renamed.columns[1:])\n",
        "attacks_drop_rows = attacks_renamed.dropna(subset=relevant_columns, how=\"all\").copy()\n",
        "# Preparing the functions for locating and adjusting the missing values.\n",
        "def print_separator(sep, num, msg):\n",
        "    print(\"\\n\")\n",
        "    print(sep * num)\n",
        "    print(f\"{msg}\")\n",
        "    print(sep * num)\n",
        "\n",
        "\n",
        "# TACTIC A: find unique values\n",
        "def look_at_unique_values(column):\n",
        "    unique_values_cutoff = 50\n",
        "    unique_values = column.unique()\n",
        "    num_unique_values = len(unique_values)\n",
        "    if num_unique_values == len(column):\n",
        "        print(f\"Each value in the column is unique (total: {num_unique_values})\")\n",
        "    elif num_unique_values < unique_values_cutoff:\n",
        "        print(f\"Less than {unique_values_cutoff} unique values:\")\n",
        "        try:\n",
        "            sorted = np.sort(unique_values)\n",
        "            print(\"Values are sorted\")\n",
        "            display(list(sorted))\n",
        "        except:\n",
        "            print(\"Could not sort values\")\n",
        "            display(list(unique_values))\n",
        "    else:\n",
        "        print(\n",
        "            f\"More than {unique_values_cutoff} unique values (total: {num_unique_values})\"\n",
        "        )\n",
        "\n",
        "\n",
        "# TACTIC B: look at the edges\n",
        "def look_at_edges(df, column_name):\n",
        "    # inner function\n",
        "    def show_head_and_tail(values):\n",
        "        num_items_to_slice = 10\n",
        "        display(list(values)[:num_items_to_slice])\n",
        "        display(list(values)[-num_items_to_slice:])\n",
        "\n",
        "    column = df[column_name]\n",
        "    unique_values = column.unique()\n",
        "    try:\n",
        "        sorted = np.sort(unique_values)\n",
        "        print(\"Unique values sorted, head and tail:\")\n",
        "        show_head_and_tail(sorted)\n",
        "    except TypeError as error:\n",
        "        print(f\"Could not sort values: {error}\")\n",
        "        print(\"..so let's try filtering NULL values and then sorting\")\n",
        "        print(\"..there could be a black sheep in the null values\")\n",
        "        non_null_uniques = df.loc[~df[column_name].isnull(), column_name].unique()\n",
        "        sorted = np.sort(non_null_uniques)\n",
        "        show_head_and_tail(sorted)\n",
        "\n",
        "\n",
        "# TACTIC C: casting to a type to see if all the values match the needed type\n",
        "def cast_to_type(column, maybe_type):\n",
        "    try:\n",
        "        column.astype(maybe_type)\n",
        "        print(f\"Casting to {maybe_type} was successful\")\n",
        "    except ValueError as error:\n",
        "        print(f\"Could not cast to {maybe_type}: {error}\")\n",
        "\n",
        "\n",
        "# TACTIC D: display the value count of the column\n",
        "def value_count(column):\n",
        "    display(column.value_counts(dropna=False))\n",
        "\n",
        "\n",
        "# FUNCTION TO CHECK THE DATAFRAME FOR ALL FOUR TACTICS\n",
        "def find_non_default_missing_values(df, column_name, maybe_type):\n",
        "    long_separator_amount = 80\n",
        "    short_separator_amount = 40\n",
        "    # Print the header\n",
        "    print_separator(\n",
        "        \"*\",\n",
        "        long_separator_amount,\n",
        "        f'Finding non default missing values for column \"{column_name}\"',\n",
        "    )\n",
        "    print(f'Column \"{column_name}\" has datatype: {df.dtypes[column_name]}')\n",
        "    column = df[column_name]\n",
        "    # A\n",
        "    print_separator(\"-\", short_separator_amount, \"A: Looking at unique values\")\n",
        "    look_at_unique_values(column)\n",
        "    # B\n",
        "    print_separator(\"-\", short_separator_amount, \"B: Sorting and looking at the edges\")\n",
        "    look_at_edges(df, column_name)\n",
        "    # C\n",
        "    print_separator(\"-\", short_separator_amount, f\"C: Casting to type: {maybe_type}\")\n",
        "    cast_to_type(column, maybe_type)\n",
        "    # D\n",
        "    print_separator(\n",
        "        \"-\",\n",
        "        short_separator_amount,\n",
        "        \"D: Looking at frequency\\nAll default-NULL values will be bunched together as NaN\",\n",
        "    )\n",
        "    value_count(column)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "# Function to replace non-default NULL values with default NULL values.\n",
        "# ⚠️ Mutates df\n",
        "def replace_value(df, column_name, missing_old, missing_new):\n",
        "    df[column_name] = df[column_name].replace({missing_old: missing_new})\n",
        "\n",
        "\n",
        "# Function to display the default NULL values in the column.\n",
        "def display_default_null_values(df, column_name):\n",
        "    nulls = df.loc[df[column_name].isnull()]\n",
        "    print(f'Number of default null values in \"{column_name}\": {len(nulls)}')\n",
        "\n",
        "\n",
        "# Easier to type\n",
        "nat = np.datetime64(\"nat\")\n"
      ],
      "metadata": {
        "id": "ltB1M1HWb8NX"
      },
      "execution_count": 441,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a deep copy of the dataframe for more clarity while working on it\n",
        "attacks_clean = attacks_drop_rows.copy(deep=True)\n",
        "\n",
        "# Lowercase all strings and strip whitespace and/or quotationmarks around strings\n",
        "attacks_clean = attacks_clean.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "attacks_clean = attacks_clean.applymap(lambda x: x.strip('\" ') if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "Bme6RygpeKm9"
      },
      "execution_count": 442,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Species\n",
        "\n",
        "First let me apologize for the amount stuff done that is probably technically not all needed for this assignment ;-)\n",
        "I had a blast cleaning op this column and I used it to practice a lot of new skills.\n",
        "Also: I recognise that in real life it would probably almost always be a waste of time to refine values that occur only a few times but I appreciated the practice ;-)"
      ],
      "metadata": {
        "id": "WxHdZoHMGxbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how everthing looks and what stands out: \n",
        "\n",
        "find_non_default_missing_values(attacks_clean, 'Species', 'string')"
      ],
      "metadata": {
        "id": "app7o0vfcppu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# First steps cleaning up:\n",
        "# Goal:\n",
        "# extract the species from the string where possible, change null values\n",
        "# where needed\n",
        "\n",
        "\n",
        "def tidy(x):\n",
        "    if pd.isna(x):\n",
        "        return None\n",
        "    elif len(x.strip()) == 0:\n",
        "        return None\n",
        "    elif \"shark\" in x:\n",
        "        return re.search(r\"(\\S+\\s*)?shark\", x).group()\n",
        "    else:\n",
        "        return f\"check: {x}\"\n",
        "\n",
        "\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].apply(tidy)\n",
        "\n",
        "# Things we can safely change to \"no species confirmed\":\n",
        "no_species = [\n",
        "    \"invalid\",\n",
        "    \"unidentified\",\n",
        "    \"questionable\",\n",
        "    \"possibly\",\n",
        "    \"not confirmed\",\n",
        "    \"unconfirmed\",\n",
        "    \"doubtful\",\n",
        "    \"captive\",\n",
        "    \"unknown\",\n",
        "    \"several\",\n",
        "    \"colored\",\n",
        "]\n",
        "\n",
        "# one or more digits followed by ' or \" followed by\n",
        "# zero or more ] followed by shark, whitespaces optional\n",
        "inches = r'\\d+\\s*([\"\\']{1,})\\s*\\]*\\s*shark'\n",
        "\n",
        "# string containing two or less letters or digits or -\n",
        "# followed by shark, whitespaces optional\n",
        "small_string = r\"^[a-z0-9-]{0,2}\\s*shark$\"\n",
        "\n",
        "# lb or kg or foot followed by zero or more ] followed by shark, whitespaces optional\n",
        "measurements = r\"(kg|lb|foot)\\s*\\]*\\s*shark\"\n",
        "\n",
        "\n",
        "def tidy_more(x):\n",
        "    if x is not None and (\n",
        "        any(word in x for word in no_species)\n",
        "        or re.search(inches, x)\n",
        "        or re.search(small_string, x)\n",
        "        or re.search(measurements, x)\n",
        "    ):\n",
        "        return None\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].apply(tidy_more)\n",
        "\n",
        "# removing quotation marks and this weird little fellas that look the same but are different:  \n",
        "def remove_weirdos(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    else:\n",
        "        return re.sub(r'[\"]+', \"\", x)\n",
        "\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].apply(remove_weirdos)\n",
        "\n",
        "# Checking all values that I previously marked as 'check'\n",
        "mask_species = attacks_clean[\"Species\"].str.contains(\"check\", na=False)\n",
        "attacks_clean[mask_species]\n",
        "\n",
        "# Changing to the right species where possible\n",
        "correct_species = [\n",
        "    \"blue pointer\",\n",
        "    \"wobbegong\",\n",
        "    \"whaler\",\n",
        "    \"hammerhead\",\n",
        "    \"porbeagle\",\n",
        "    \"whitetip\",\n",
        "    \"horn\",\n",
        "]\n",
        "\n",
        "def correct_checks(x):\n",
        "    if x is not None and (\"check\" in x):\n",
        "        for word in correct_species:\n",
        "            if word in x:\n",
        "                return f\"{word} shark\"\n",
        "        else:\n",
        "            return None\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].apply(lambda x: correct_checks(x))\n",
        "\n",
        "\n",
        "# Last crumbs to clean up:\n",
        "# attacks_clean['Species'].value_counts().head(50)\n",
        "# attacks_clean['Species'].value_counts().tail(50)\n",
        "\n",
        "useless = [\n",
        "    \"large shark\",\n",
        "    \"female shark\",\n",
        "    \"grey shark\",\n",
        "    \"two shark\",\n",
        "    \"the shark\",\n",
        "    \"from shark\",\n",
        "    \"little shark\",\n",
        "    \"larger shark\",\n",
        "    \"red shark\",\n",
        "    \"young shark\",\n",
        "    \"for shark\",\n",
        "    \"metre shark\",\n",
        "    \"juvenile shark\",\n",
        "    \"gray shark\",\n",
        "    \"finned shark\",\n",
        "]\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].replace(\n",
        "    dict.fromkeys(useless, None)\n",
        ")\n",
        "\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].replace(\n",
        "    dict.fromkeys(\n",
        "        [\"seven-gill shark\", \"7-gill shark\", \"sevengill  shark\"], \"sevengill shark\"\n",
        "    )\n",
        ")\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].replace(\n",
        "    dict.fromkeys([\"black-tipped shark\", \"blacktip  shark\"], \"blacktip shark\")\n",
        ")\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].replace(\n",
        "    {\"sand shark\": \"sandshark\", \"zambesi shark\": \"zambezi shark\"}\n",
        ")\n",
        "\n",
        "# Those last values are probably useless but since there is no way to definitively know if they represent a real species or not I'll leave them as is for now.\n",
        "# attacks_clean['Species'].value_counts().head(50)\n",
        "# attacks_clean['Species'].value_counts().tail(50)"
      ],
      "metadata": {
        "id": "vPU6Sx3Pho0h"
      },
      "execution_count": 443,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Case"
      ],
      "metadata": {
        "id": "z4xFIr7_lh_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how everything looks and what stands out: \n",
        "find_non_default_missing_values(attacks_clean, \"Case\", \"string\")"
      ],
      "metadata": {
        "id": "erCe7JgRliJe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# There are some duplicate values. \n",
        "# Checking what they mean: \n",
        "\n",
        "# attacks_clean[attacks_clean.duplicated('Case', keep=False)]\n",
        "\n",
        "# They might indicatie cases where more then one person was attacked. \n",
        "# The columns regarding location and time might help here if needed but since \n",
        "# there are few and they seem irrelevant to our questions I will leave them for now"
      ],
      "metadata": {
        "id": "qwp5aONal2ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Type"
      ],
      "metadata": {
        "id": "zSFES5JHcf8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how everything looks and what stands out: \n",
        "find_non_default_missing_values(attacks_clean, \"Type\", \"string\")"
      ],
      "metadata": {
        "id": "gK24EjdGdR5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace a few different values that all mean the same plus replace np.nan.\n",
        "attacks_clean[\"Type\"] = attacks_clean[\"Type\"].replace({\"boating\" : \"boat\", \"boatomg\" : \"boat\", np.nan: None})\n"
      ],
      "metadata": {
        "id": "xofMku-Mcda6"
      },
      "execution_count": 444,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Activity"
      ],
      "metadata": {
        "id": "I_2SWnDVdbVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Column Activity\n",
        "# Checking how everthing looks and what stands out: \n",
        "find_non_default_missing_values(attacks_clean, \"Activity\", \"string\")"
      ],
      "metadata": {
        "id": "hU4Ws2cfda86"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replacing the some things that stand out right away\n",
        "attacks_clean[\"Activity\"] = attacks_clean[\"Activity\"].replace({\".\": None, np.nan: None})"
      ],
      "metadata": {
        "id": "RvaBdkw7mXLz"
      },
      "execution_count": 445,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking all none - unique values: \n",
        "# attacks_clean[attacks_clean.duplicated(\"Activity\", keep=False)]\n",
        "\n",
        "# Looking at the head and tail of the column to see what stands out:\n",
        "# attacks_clean[\"Activity\"].value_counts().head(60)\n",
        "# attacks2[\"Activity\"].value_counts().tail(50)"
      ],
      "metadata": {
        "id": "ovuZERyynAZ7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Combining different spellings of the same value\n",
        "attacks_clean[\"Activity\"] = attacks_clean[\"Activity\"].replace(\n",
        "    dict.fromkeys([\"boogie boarding\", \"paddle boarding\", \"body-boarding\", \"body boarding\", \"paddle-boarding\"], \"bodyboarding\")\n",
        ")\n",
        "\n",
        "# Taking the top 60 values as categories and dividing the other values, where possible,\n",
        "# into those categories.\n",
        "\n",
        "top_activities = list(attacks_clean[\"Activity\"].value_counts().head(60).index)\n",
        "\n",
        "def activities_checks(x):\n",
        "    if x is not None:\n",
        "      for word in top_activities:\n",
        "          if word in x:\n",
        "              return word\n",
        "      else:\n",
        "          return x\n",
        "\n",
        "attacks_clean[\"Activity\"] = attacks_clean[\"Activity\"].apply(lambda x: activities_checks(x))"
      ],
      "metadata": {
        "id": "YAq5pTNCFqnh"
      },
      "execution_count": 446,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Fatal"
      ],
      "metadata": {
        "id": "KxO2VYB3WrmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how everything looks and what stands out: \n",
        "find_non_default_missing_values(attacks_clean, \"Outcome\", \"string\")"
      ],
      "metadata": {
        "id": "NY_GuCpSWwXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing values to corresponding categories\n",
        "attacks_clean[\"Outcome\"] = attacks_clean[\"Outcome\"].replace({\"n\" : \"nonfatal\", \"y\" : \"fatal\", \"m\" : \"unknown\", \"2017\": \"unknown\", np.nan : None})"
      ],
      "metadata": {
        "id": "YPDOSx_-W-rj"
      },
      "execution_count": 447,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Age"
      ],
      "metadata": {
        "id": "lneg2rTgbqew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how everything looks and what stands out: \n",
        "find_non_default_missing_values(attacks_clean, \"Age\", \"string\")"
      ],
      "metadata": {
        "id": "0stjOT8_bnio"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# change NaN to None\n",
        "attacks_clean[\"Age\"] = attacks_clean[\"Age\"].replace({np.nan : None, \"\": None})\n",
        "\n",
        "# convert non-null values to string\n",
        "non_null = attacks_clean[\"Age\"].notnull()\n",
        "attacks_clean.loc[non_null, \"Age\"] = attacks_clean.loc[non_null, \"Age\"].astype(str)\n",
        "\n",
        "# Sorting ages by range\n",
        "child = [str(i) for i in range(0, 13)]\n",
        "adolescent = [str(i) for i in range(13, 18)]\n",
        "adult = [str(i) for i in range(18, 130)]\n",
        "\n",
        "# Sorting all the values that describe the age in words\n",
        "# As noticed when evaluating what couldn't be done automatically\n",
        "child_words = [\"child\", \"2 to 3 months\", \"9 months\", \"18 months\", \"2½\", \"both 11\", \"6½\"]\n",
        "adolescent_words = [\"teen\", \"teens\"]\n",
        "adult_words = [\"adult\" , \"middle-age\", \"(adult)\", \"20s\", \"30s\", \"40s\", \"50s\", \"elderly\", \"60's\", \"60s\", \"mid-20s\", \"mid-30s\", \"ca. 33\"]\n",
        "\n",
        "# Sorting into age groups, also tackling values that mention multiple ages\n",
        "# Making a new column for the age_groups\n",
        "def age_groups(age):\n",
        "    if age is None:\n",
        "        return age\n",
        "    else:\n",
        "        age_list = re.split(r'[,&\\s?]|(?:\\s*(?:to|or)\\s*)', age) # split into seperate ages where needed\n",
        "        age_list = list(filter(None, age_list))  # remove any empty strings  \n",
        "        if all(word in child for word in age_list) or age in child_words:\n",
        "            return \"child\"\n",
        "        elif all(word in adolescent for word in age_list) or age in adolescent_words:\n",
        "            return \"adolescent\"\n",
        "        elif all(word in adult for word in age_list) or age in adult_words:\n",
        "            return \"adult\"\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "attacks_clean.insert(4, \"Age_group\", attacks_clean[\"Age\"].apply(age_groups))"
      ],
      "metadata": {
        "id": "m9laS1eeJ-jN"
      },
      "execution_count": 448,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Injury"
      ],
      "metadata": {
        "id": "5hqIPB3DGI1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how everything looks and what stands out: \n",
        "find_non_default_missing_values(attacks_clean, \"Injury\", \"string\")"
      ],
      "metadata": {
        "id": "THQ0tcVXHs-4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# temporary copy:\n",
        "copy = attacks_clean.copy(deep=True)"
      ],
      "metadata": {
        "id": "GloF8sbvPEuK"
      },
      "execution_count": 605,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Changing Nan to None\n",
        "copy[\"Injury\"] = copy[\"Injury\"].replace({np.nan : None, \"\": None})"
      ],
      "metadata": {
        "id": "Tw6KDaDUhbsH"
      },
      "execution_count": 606,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a new column and filling it with whith the values of \"Injury\".\n",
        "copy.insert(5, \"Injury_2\", copy[\"Injury\"])"
      ],
      "metadata": {
        "id": "0OVsX3k2Ozke"
      },
      "execution_count": 607,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Removing some words for better sorting\n",
        "copy[\"Injury_2\"] = copy[\"Injury_2\"].str.replace(r\"\\bleft\\b|\\bright\\b|\\bminor\\b|\\bsevere\\b|\\blower\\b|\\bto dorsum of\\b\", \"\", regex=True).str.strip()\n",
        "# copy[\"Injury_2\"] = copy[\"Injury_2\"].str.replace(r\"\\bleft\\b|\\bright\\b\", \"\", regex=True).str.strip()\n",
        "\n",
        "# Correcting some words for better sorting\n",
        "copy[\"Injury_2\"] = copy[\"Injury_2\"].replace({\"injury\":\"injuries\"})\n",
        "\n",
        "# Replace values that contain \"fatal\" with just \"fatal\"\n",
        "copy[\"Injury_2\"] = copy[\"Injury_2\"].str.replace(r\".*\\bfatal\\b.*\", \"fatal\", regex=True)\n",
        "\n",
        "# Replace values that contain \"no injury\" with just \"no injury\"\n",
        "copy[\"Injury_2\"] = copy[\"Injury_2\"].str.replace(r\".*\\bno injury\\b.*\", \"no injury\", regex=True)"
      ],
      "metadata": {
        "id": "gTvUnFyNYqEV"
      },
      "execution_count": 608,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Replace \"laceration\" with \"lacerations\"\n",
        "copy[\"Injury_2\"] = copy[\"Injury_2\"].str.replace(r\"\\blaceration\\b\", \"lacerations\", regex=True)\n",
        "# Replace \"lacerated\" with \"lacerations to\"\n",
        "copy[\"Injury_2\"] = copy[\"Injury_2\"].str.replace(r\"\\blacerated\\b\", \"lacerations to\", regex=True)\n",
        "# copy[\"Injury_2\"] = copy[\"Injury_2\"].str.replace(r'\\b(lacerated|laceration)\\b', 'lacerations to', regex=True)\n"
      ],
      "metadata": {
        "id": "O1z7Ltmof2C7"
      },
      "execution_count": 609,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If \"lacerations to\" is present: place it at the front of the string\n",
        "\n",
        "def move_lacerations_to_front(injury):\n",
        "    if injury is None:\n",
        "        return injury\n",
        "    else: \n",
        "      if \"lacerations to\" in injury:\n",
        "         return \"lacerations to \" + injury.replace(\"lacerations to\", \"\").strip()\n",
        "      else:\n",
        "         return injury.strip()\n",
        "\n",
        "# Apply the function to the \"Injury_2\" column\n",
        "copy[\"Injury_2\"] = copy[\"Injury_2\"].apply(move_lacerations_to_front)\n"
      ],
      "metadata": {
        "id": "CY4GPP-CclGd"
      },
      "execution_count": 610,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove double whitespaces\n",
        "copy[\"Injury_2\"] = copy[\"Injury_2\"].str.replace(r\"\\s{2,}\", \" \", regex=True)"
      ],
      "metadata": {
        "id": "Pp0C4DDud40C"
      },
      "execution_count": 611,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "copy[\"Injury_2\"].value_counts().head(60\n",
        "                                    )"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sXON0W80cmPC",
        "outputId": "1e0834f8-b1eb-4a0e-f98a-aa9ba8713693"
      },
      "execution_count": 612,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fatal                                            1360\n",
              "no injury                                         804\n",
              "foot bitten                                       178\n",
              "lacerations to foot                               139\n",
              "leg bitten                                        120\n",
              "lacerations to leg                                111\n",
              "survived                                           97\n",
              "lacerations to hand                                58\n",
              "calf bitten                                        49\n",
              "hand bitten                                        47\n",
              "thigh bitten                                       46\n",
              "lacerations to thigh                               45\n",
              "no details                                         43\n",
              "lacerations to calf                                41\n",
              "arm bitten                                         41\n",
              "lacerations to arm                                 37\n",
              "injuries                                           37\n",
              "ankle bitten                                       24\n",
              "lacerations to forearm                             24\n",
              "foot severed                                       19\n",
              "puncture wounds to foot                            16\n",
              "lacerations to ankle                               16\n",
              "forearm bitten                                     16\n",
              "leg injured                                        15\n",
              "leg severed                                        15\n",
              "legs bitten                                        14\n",
              "lacerations to foot & ankle                        14\n",
              "leg bitten, surgically amputated                   14\n",
              "heel bitten                                        14\n",
              "injury to foot                                     13\n",
              "hand severed                                       13\n",
              "lacerations to knee                                12\n",
              "leg severely bitten                                10\n",
              "injury to hand                                     10\n",
              "probable drowning & scavenging                     10\n",
              "leg & foot bitten                                  10\n",
              "arm severed                                        10\n",
              "lacerations to heel                                10\n",
              "injury to leg                                       9\n",
              "shoulder bitten                                     9\n",
              "lacerations to leg & foot                           9\n",
              "knee bitten                                         9\n",
              "ankle & foot bitten                                 9\n",
              "lacerations to fingers                              8\n",
              "elbow bitten                                        8\n",
              "hand bitten by hooked shark provoked incident       7\n",
              "foot injured                                        7\n",
              "lacerations to ankle & foot                         7\n",
              "torso bitten                                        7\n",
              "hand injured                                        7\n",
              "lacerations foot                                    6\n",
              "swim fin bitten                                     6\n",
              "injury to calf                                      6\n",
              "leg severely bitten, surgically amputated           6\n",
              "lacerations                                         6\n",
              "injuries to foot                                    6\n",
              "lacerations to shin                                 6\n",
              "foot & ankle bitten                                 6\n",
              "lacerations to foot and ankle                       6\n",
              "lacerations to legs                                 6\n",
              "Name: Injury_2, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 612
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Looking at the head and tail of the column to see what stands out:\n",
        "copy[\"Injury_2\"].value_counts().head(10)\n",
        "# copy[\"Injury_2\"].value_counts().tail(50)\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "q_oLJFUbH8HY",
        "outputId": "c50b1ba5-4f45-4d23-b6a6-751245b52e00"
      },
      "execution_count": 511,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "fatal             815\n",
              "no injury         758\n",
              "foot bitten       178\n",
              "leg bitten        109\n",
              "survived           97\n",
              "calf bitten        49\n",
              "thigh bitten       46\n",
              "hand bitten        46\n",
              "no details         43\n",
              "foot lacerated     43\n",
              "Name: Injury_2, dtype: int64"
            ]
          },
          "metadata": {},
          "execution_count": 511
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# copy[\"Injury_2\"] = copy[\"Injury_2\"].str.replace"
      ],
      "metadata": {
        "id": "mdaRiM0ncN_4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Remove double whitespaces\n",
        "copy[\"Injury_2\"] = copy[\"Injury_2\"].str.replace(r\"\\s{2,}\", \" \")"
      ],
      "metadata": {
        "id": "jp0RX1bERK38"
      },
      "execution_count": 481,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Check column 'Injury' for missing values\n",
        "\n",
        "# find_non_default_missing_values(attacks_clean, 'Injury', 'string')\n",
        "\n",
        "# DONE for now"
      ],
      "metadata": {
        "id": "dDBR03O1mZAh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}