{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "mount_file_id": "1qZMXOF2HeaNzOgS0Q_R7qr8VoCRe8Xc6",
      "authorship_tag": "ABX9TyPWsxj723AXXsd1xx64fjuF",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/MevrouwHelderder/Assignments/blob/main/Assignment_Shark_Attack.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%load_ext google.colab.data_table"
      ],
      "metadata": {
        "id": "-GTLoS1qovCO"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Importing the essentials\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import re\n",
        "\n",
        "# importing the dataframe\n",
        "path = \"/content/drive/MyDrive/attacks.csv\"\n",
        "attacks = pd.read_csv(path, encoding=\"ISO-8859-1\")\n",
        "# Dropping columns.\n",
        "attacks_drop_columns = attacks.drop(\n",
        "    columns=[\n",
        "        \"Date\",\n",
        "        \"Year\",\n",
        "        \"Country\",\n",
        "        \"Area\",\n",
        "        \"Location\",\n",
        "        \"Name\",\n",
        "        \"Sex \",\n",
        "        \"Time\",\n",
        "        \"Investigator or Source\",\n",
        "        \"pdf\",\n",
        "        \"href formula\",\n",
        "        \"href\",\n",
        "        \"Case Number.1\",\n",
        "        \"Case Number.2\",\n",
        "        \"original order\",\n",
        "        \"Unnamed: 22\",\n",
        "        \"Unnamed: 23\",\n",
        "    ]\n",
        ")\n",
        "# Renaming columns.\n",
        "attacks_renamed = attacks_drop_columns.rename(\n",
        "    columns={\"Case Number\": \"Case\", \"Fatal (Y/N)\": \"Outcome\", \"Species \": \"Species\", \"Ages\": \"Age\"}\n",
        ")\n",
        "# Dropping rows.\n",
        "# Making a copy to prevent view vs copy issues later on.\n",
        "relevant_columns = list(attacks_renamed.columns[1:])\n",
        "attacks_drop_rows = attacks_renamed.dropna(subset=relevant_columns, how=\"all\").copy()\n",
        "# Preparing the functions for locating and adjusting the missing values.\n",
        "def print_separator(sep, num, msg):\n",
        "    print(\"\\n\")\n",
        "    print(sep * num)\n",
        "    print(f\"{msg}\")\n",
        "    print(sep * num)\n",
        "\n",
        "\n",
        "# TACTIC A: find unique values\n",
        "def look_at_unique_values(column):\n",
        "    unique_values_cutoff = 50\n",
        "    unique_values = column.unique()\n",
        "    num_unique_values = len(unique_values)\n",
        "    if num_unique_values == len(column):\n",
        "        print(f\"Each value in the column is unique (total: {num_unique_values})\")\n",
        "    elif num_unique_values < unique_values_cutoff:\n",
        "        print(f\"Less than {unique_values_cutoff} unique values:\")\n",
        "        try:\n",
        "            sorted = np.sort(unique_values)\n",
        "            print(\"Values are sorted\")\n",
        "            display(list(sorted))\n",
        "        except:\n",
        "            print(\"Could not sort values\")\n",
        "            display(list(unique_values))\n",
        "    else:\n",
        "        print(\n",
        "            f\"More than {unique_values_cutoff} unique values (total: {num_unique_values})\"\n",
        "        )\n",
        "\n",
        "\n",
        "# TACTIC B: look at the edges\n",
        "def look_at_edges(df, column_name):\n",
        "    # inner function\n",
        "    def show_head_and_tail(values):\n",
        "        num_items_to_slice = 10\n",
        "        display(list(values)[:num_items_to_slice])\n",
        "        display(list(values)[-num_items_to_slice:])\n",
        "\n",
        "    column = df[column_name]\n",
        "    unique_values = column.unique()\n",
        "    try:\n",
        "        sorted = np.sort(unique_values)\n",
        "        print(\"Unique values sorted, head and tail:\")\n",
        "        show_head_and_tail(sorted)\n",
        "    except TypeError as error:\n",
        "        print(f\"Could not sort values: {error}\")\n",
        "        print(\"..so let's try filtering NULL values and then sorting\")\n",
        "        print(\"..there could be a black sheep in the null values\")\n",
        "        non_null_uniques = df.loc[~df[column_name].isnull(), column_name].unique()\n",
        "        sorted = np.sort(non_null_uniques)\n",
        "        show_head_and_tail(sorted)\n",
        "\n",
        "\n",
        "# TACTIC C: casting to a type to see if all the values match the needed type\n",
        "def cast_to_type(column, maybe_type):\n",
        "    try:\n",
        "        column.astype(maybe_type)\n",
        "        print(f\"Casting to {maybe_type} was successful\")\n",
        "    except ValueError as error:\n",
        "        print(f\"Could not cast to {maybe_type}: {error}\")\n",
        "\n",
        "\n",
        "# TACTIC D: display the value count of the column\n",
        "def value_count(column):\n",
        "    display(column.value_counts(dropna=False))\n",
        "\n",
        "\n",
        "# FUNCTION TO CHECK THE DATAFRAME FOR ALL FOUR TACTICS\n",
        "def find_non_default_missing_values(df, column_name, maybe_type):\n",
        "    long_separator_amount = 80\n",
        "    short_separator_amount = 40\n",
        "    # Print the header\n",
        "    print_separator(\n",
        "        \"*\",\n",
        "        long_separator_amount,\n",
        "        f'Finding non default missing values for column \"{column_name}\"',\n",
        "    )\n",
        "    print(f'Column \"{column_name}\" has datatype: {df.dtypes[column_name]}')\n",
        "    column = df[column_name]\n",
        "    # A\n",
        "    print_separator(\"-\", short_separator_amount, \"A: Looking at unique values\")\n",
        "    look_at_unique_values(column)\n",
        "    # B\n",
        "    print_separator(\"-\", short_separator_amount, \"B: Sorting and looking at the edges\")\n",
        "    look_at_edges(df, column_name)\n",
        "    # C\n",
        "    print_separator(\"-\", short_separator_amount, f\"C: Casting to type: {maybe_type}\")\n",
        "    cast_to_type(column, maybe_type)\n",
        "    # D\n",
        "    print_separator(\n",
        "        \"-\",\n",
        "        short_separator_amount,\n",
        "        \"D: Looking at frequency\\nAll default-NULL values will be bunched together as NaN\",\n",
        "    )\n",
        "    value_count(column)\n",
        "    print(\"\\n\")\n",
        "\n",
        "\n",
        "# Function to replace non-default NULL values with default NULL values.\n",
        "# ⚠️ Mutates df\n",
        "def replace_value(df, column_name, missing_old, missing_new):\n",
        "    df[column_name] = df[column_name].replace({missing_old: missing_new})\n",
        "\n",
        "\n",
        "# Function to display the default NULL values in the column.\n",
        "def display_default_null_values(df, column_name):\n",
        "    nulls = df.loc[df[column_name].isnull()]\n",
        "    print(f'Number of default null values in \"{column_name}\": {len(nulls)}')\n",
        "\n",
        "\n",
        "# Easier to type\n",
        "nat = np.datetime64(\"nat\")\n"
      ],
      "metadata": {
        "id": "ltB1M1HWb8NX"
      },
      "execution_count": 441,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Making a deep copy of the dataframe for more clarity while working on it\n",
        "attacks_clean = attacks_drop_rows.copy(deep=True)\n",
        "\n",
        "# Lowercase all strings and strip whitespace and/or quotationmarks around strings\n",
        "attacks_clean = attacks_clean.applymap(lambda x: x.lower() if isinstance(x, str) else x)\n",
        "attacks_clean = attacks_clean.applymap(lambda x: x.strip('\" ') if isinstance(x, str) else x)"
      ],
      "metadata": {
        "id": "Bme6RygpeKm9"
      },
      "execution_count": 442,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Species\n",
        "\n",
        "First let me apologize for the amount stuff done that is probably technically not all needed for this assignment ;-)\n",
        "I had a blast cleaning op this column and I used it to practice a lot of new skills.\n",
        "Also: I recognise that in real life it would probably almost always be a waste of time to refine values that occur only a few times but I appreciated the practice ;-)"
      ],
      "metadata": {
        "id": "WxHdZoHMGxbr"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how everthing looks and what stands out: \n",
        "# find_non_default_missing_values(attacks_clean, 'Species', 'string')\n",
        "\n",
        "# First steps cleaning up:\n",
        "# Goal:\n",
        "# extract the species from the string where possible, change null values\n",
        "# where needed\n",
        "\n",
        "\n",
        "def tidy(x):\n",
        "    if pd.isna(x):\n",
        "        return None\n",
        "    elif len(x.strip()) == 0:\n",
        "        return None\n",
        "    elif \"shark\" in x:\n",
        "        return re.search(r\"(\\S+\\s*)?shark\", x).group()\n",
        "    else:\n",
        "        return f\"check: {x}\"\n",
        "\n",
        "\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].apply(tidy)\n",
        "\n",
        "# Things we can safely change to \"no species confirmed\":\n",
        "no_species = [\n",
        "    \"invalid\",\n",
        "    \"unidentified\",\n",
        "    \"questionable\",\n",
        "    \"possibly\",\n",
        "    \"not confirmed\",\n",
        "    \"unconfirmed\",\n",
        "    \"doubtful\",\n",
        "    \"captive\",\n",
        "    \"unknown\",\n",
        "    \"several\",\n",
        "    \"colored\",\n",
        "]\n",
        "\n",
        "# one or more digits followed by ' or \" followed by\n",
        "# zero or more ] followed by shark, whitespaces optional\n",
        "inches = r'\\d+\\s*([\"\\']{1,})\\s*\\]*\\s*shark'\n",
        "\n",
        "# string containing two or less letters or digits or -\n",
        "# followed by shark, whitespaces optional\n",
        "small_string = r\"^[a-z0-9-]{0,2}\\s*shark$\"\n",
        "\n",
        "# lb or kg or foot followed by zero or more ] followed by shark, whitespaces optional\n",
        "measurements = r\"(kg|lb|foot)\\s*\\]*\\s*shark\"\n",
        "\n",
        "\n",
        "def tidy_more(x):\n",
        "    if x is not None and (\n",
        "        any(word in x for word in no_species)\n",
        "        or re.search(inches, x)\n",
        "        or re.search(small_string, x)\n",
        "        or re.search(measurements, x)\n",
        "    ):\n",
        "        return None\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].apply(tidy_more)\n",
        "\n",
        "# removing quotation marks and this weird little fellas that look the same but are different:  \n",
        "def remove_weirdos(x):\n",
        "    if x is None:\n",
        "        return None\n",
        "    else:\n",
        "        return re.sub(r'[\"]+', \"\", x)\n",
        "\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].apply(remove_weirdos)\n",
        "\n",
        "# Checking all values that I previously marked as 'check'\n",
        "mask_species = attacks_clean[\"Species\"].str.contains(\"check\", na=False)\n",
        "attacks_clean[mask_species]\n",
        "\n",
        "# Changing to the right species where possible\n",
        "correct_species = [\n",
        "    \"blue pointer\",\n",
        "    \"wobbegong\",\n",
        "    \"whaler\",\n",
        "    \"hammerhead\",\n",
        "    \"porbeagle\",\n",
        "    \"whitetip\",\n",
        "    \"horn\",\n",
        "]\n",
        "\n",
        "def correct_checks(x):\n",
        "    if x is not None and (\"check\" in x):\n",
        "        for word in correct_species:\n",
        "            if word in x:\n",
        "                return f\"{word} shark\"\n",
        "        else:\n",
        "            return None\n",
        "    else:\n",
        "        return x\n",
        "\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].apply(lambda x: correct_checks(x))\n",
        "\n",
        "\n",
        "# Last crumbs to clean up:\n",
        "# attacks_clean['Species'].value_counts().head(50)\n",
        "# attacks_clean['Species'].value_counts().tail(50)\n",
        "\n",
        "useless = [\n",
        "    \"large shark\",\n",
        "    \"female shark\",\n",
        "    \"grey shark\",\n",
        "    \"two shark\",\n",
        "    \"the shark\",\n",
        "    \"from shark\",\n",
        "    \"little shark\",\n",
        "    \"larger shark\",\n",
        "    \"red shark\",\n",
        "    \"young shark\",\n",
        "    \"for shark\",\n",
        "    \"metre shark\",\n",
        "    \"juvenile shark\",\n",
        "    \"gray shark\",\n",
        "    \"finned shark\",\n",
        "]\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].replace(\n",
        "    dict.fromkeys(useless, None)\n",
        ")\n",
        "\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].replace(\n",
        "    dict.fromkeys(\n",
        "        [\"seven-gill shark\", \"7-gill shark\", \"sevengill  shark\"], \"sevengill shark\"\n",
        "    )\n",
        ")\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].replace(\n",
        "    dict.fromkeys([\"black-tipped shark\", \"blacktip  shark\"], \"blacktip shark\")\n",
        ")\n",
        "attacks_clean[\"Species\"] = attacks_clean[\"Species\"].replace(\n",
        "    {\"sand shark\": \"sandshark\", \"zambesi shark\": \"zambezi shark\"}\n",
        ")\n",
        "\n",
        "# Those last values are probably useless but since there is no way to definitively know if they represent a real species or not I'll leave them as is for now.\n",
        "# attacks_clean['Species'].value_counts().head(50)\n",
        "# attacks_clean['Species'].value_counts().tail(50)"
      ],
      "metadata": {
        "id": "vPU6Sx3Pho0h"
      },
      "execution_count": 443,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Case"
      ],
      "metadata": {
        "id": "z4xFIr7_lh_1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how everything looks and what stands out: \n",
        "# find_non_default_missing_values(attacks_clean, \"Case\", \"string\")\n",
        "\n",
        "# There are some duplicate values. \n",
        "# Checking what they mean: \n",
        "\n",
        "# attacks_clean[attacks_clean.duplicated('Case', keep=False)]\n",
        "\n",
        "# They might indicatie cases where more then one person was attacked. \n",
        "# The columns regarding location and time might help here if needed but since \n",
        "# there are few and they seem irrelevant to our questions I will leave them for now"
      ],
      "metadata": {
        "id": "qwp5aONal2ZA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Type"
      ],
      "metadata": {
        "id": "zSFES5JHcf8J"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how everything looks and what stands out: \n",
        "# find_non_default_missing_values(attacks_clean, \"Type\", \"string\")\n",
        "\n",
        "# Replace a few different values that all mean the same plus replace np.nan.\n",
        "attacks_clean[\"Type\"] = attacks_clean[\"Type\"].replace({\"boating\" : \"boat\", \"boatomg\" : \"boat\", np.nan: None})"
      ],
      "metadata": {
        "id": "gK24EjdGdR5J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Activity"
      ],
      "metadata": {
        "id": "I_2SWnDVdbVf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Column Activity\n",
        "# Checking how everthing looks and what stands out: \n",
        "# find_non_default_missing_values(attacks_clean, \"Activity\", \"string\")\n",
        "\n",
        "# Replacing the some things that stand out right away\n",
        "attacks_clean[\"Activity\"] = attacks_clean[\"Activity\"].replace({\".\": None, np.nan: None})\n",
        "\n",
        "# Checking all none - unique values: \n",
        "# attacks_clean[attacks_clean.duplicated(\"Activity\", keep=False)]\n",
        "\n",
        "# Looking at the head and tail of the column to see what stands out:\n",
        "# attacks_clean[\"Activity\"].value_counts().head(60)\n",
        "# attacks2[\"Activity\"].value_counts().tail(50)\n",
        "\n",
        "# Combining different spellings of the same value\n",
        "attacks_clean[\"Activity\"] = attacks_clean[\"Activity\"].replace(\n",
        "    dict.fromkeys([\"boogie boarding\", \"paddle boarding\", \"body-boarding\", \"body boarding\", \"paddle-boarding\"], \"bodyboarding\")\n",
        ")\n",
        "\n",
        "# Taking the top 60 values as categories and dividing the other values, where possible,\n",
        "# into those categories.\n",
        "\n",
        "top_activities = list(attacks_clean[\"Activity\"].value_counts().head(60).index)\n",
        "\n",
        "def activities_checks(x):\n",
        "    if x is not None:\n",
        "      for word in top_activities:\n",
        "          if word in x:\n",
        "              return word\n",
        "      else:\n",
        "          return x\n",
        "\n",
        "attacks_clean[\"Activity\"] = attacks_clean[\"Activity\"].apply(lambda x: activities_checks(x))"
      ],
      "metadata": {
        "id": "YAq5pTNCFqnh"
      },
      "execution_count": 446,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Fatal"
      ],
      "metadata": {
        "id": "KxO2VYB3WrmW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how everything looks and what stands out: \n",
        "# find_non_default_missing_values(attacks_clean, \"Outcome\", \"string\")\n",
        "\n",
        "# Changing values to corresponding categories\n",
        "attacks_clean[\"Outcome\"] = attacks_clean[\"Outcome\"].replace({\"n\" : \"nonfatal\", \"y\" : \"fatal\", \"m\" : \"unknown\", \"2017\": \"unknown\", np.nan : None})"
      ],
      "metadata": {
        "id": "NY_GuCpSWwXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Age"
      ],
      "metadata": {
        "id": "lneg2rTgbqew"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how everything looks and what stands out: \n",
        "# find_non_default_missing_values(attacks_clean, \"Age\", \"string\")\n",
        "\n",
        "# change NaN to None\n",
        "attacks_clean[\"Age\"] = attacks_clean[\"Age\"].replace({np.nan : None, \"\": None})\n",
        "\n",
        "# convert non-null values to string\n",
        "non_null = attacks_clean[\"Age\"].notnull()\n",
        "attacks_clean.loc[non_null, \"Age\"] = attacks_clean.loc[non_null, \"Age\"].astype(str)\n",
        "\n",
        "# Sorting ages by range\n",
        "child = [str(i) for i in range(0, 13)]\n",
        "adolescent = [str(i) for i in range(13, 18)]\n",
        "adult = [str(i) for i in range(18, 130)]\n",
        "\n",
        "# Sorting all the values that describe the age in words\n",
        "# As noticed when evaluating what couldn't be done automatically\n",
        "child_words = [\"child\", \"2 to 3 months\", \"9 months\", \"18 months\", \"2½\", \"both 11\", \"6½\"]\n",
        "adolescent_words = [\"teen\", \"teens\"]\n",
        "adult_words = [\"adult\" , \"middle-age\", \"(adult)\", \"20s\", \"30s\", \"40s\", \"50s\", \"elderly\", \"60's\", \"60s\", \"mid-20s\", \"mid-30s\", \"ca. 33\"]\n",
        "\n",
        "# Sorting into age groups, also tackling values that mention multiple ages\n",
        "# Making a new column for the age_groups\n",
        "def age_groups(age):\n",
        "    if age is None:\n",
        "        return age\n",
        "    else:\n",
        "        age_list = re.split(r'[,&\\s?]|(?:\\s*(?:to|or)\\s*)', age) # split into seperate ages where needed\n",
        "        age_list = list(filter(None, age_list))  # remove any empty strings  \n",
        "        if all(word in child for word in age_list) or age in child_words:\n",
        "            return \"child\"\n",
        "        elif all(word in adolescent for word in age_list) or age in adolescent_words:\n",
        "            return \"adolescent\"\n",
        "        elif all(word in adult for word in age_list) or age in adult_words:\n",
        "            return \"adult\"\n",
        "        else:\n",
        "            return None\n",
        "\n",
        "attacks_clean.insert(4, \"Age_group\", attacks_clean[\"Age\"].apply(age_groups))"
      ],
      "metadata": {
        "id": "m9laS1eeJ-jN"
      },
      "execution_count": 448,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Column: Injury"
      ],
      "metadata": {
        "id": "5hqIPB3DGI1Q"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Checking how everything looks and what stands out: \n",
        "# find_non_default_missing_values(attacks_clean, \"Injury\", \"string\")\n",
        "\n",
        "# Changing Nan to None\n",
        "attacks_clean[\"Injury\"] = attacks_clean[\"Injury\"].replace({np.nan : None, \"\": None})\n",
        "\n",
        "# Removing some words for better sorting\n",
        "attacks_clean[\"Injury\"] = attacks_clean[\"Injury\"].str.replace(r\"\\bleft\\b|\\bright\\b|\\bminor\\b|\\bsevere\\b|\\blower\\b|\\bto dorsum of\\b\", \"\", regex=True).str.strip()\n",
        "\n",
        "# Correcting some words for better sorting\n",
        "attacks_clean[\"Injury\"] = attacks_clean[\"Injury\"].replace({\"injury\":\"injuries\"})\n",
        "\n",
        "# Replace values that contain \"fatal\" with just \"fatal\"\n",
        "attacks_clean[\"Injury\"] = attacks_clean[\"Injury\"].str.replace(r\".*\\bfatal\\b.*\", \"fatal\", regex=True)\n",
        "\n",
        "# Replace values that contain \"no injury\" with just \"no injury\"\n",
        "attacks_clean[\"Injury\"] = attacks_clean[\"Injury\"].str.replace(r\".*\\bno injury\\b.*\", \"no injury\", regex=True)\n",
        "\n",
        "# Sorting laceration/lacerated/etc.\n",
        "\n",
        "# Replace \"laceration\" with \"lacerations\"\n",
        "attacks_clean[\"Injury\"] = attacks_clean[\"Injury\"].str.replace(r\"\\blaceration\\b\", \"lacerations\", regex=True)\n",
        "# Replace \"lacerated\" with \"lacerations to\"\n",
        "attacks_clean[\"Injury\"] = attacks_clean[\"Injury\"].str.replace(r\"\\blacerated\\b\", \"lacerations to\", regex=True)\n",
        "\n",
        "# If \"lacerations to\" is present: place it at the front of the string\n",
        "def move_lacerations_to_front(injury):\n",
        "    if injury is None:\n",
        "        return injury\n",
        "    else: \n",
        "      if \"lacerations to\" in injury:\n",
        "         return \"lacerations to \" + injury.replace(\"lacerations to\", \"\").strip()\n",
        "      else:\n",
        "         return injury.strip()\n",
        "\n",
        "# Apply the function to the \"Injury_2\" column\n",
        "attacks_clean[\"Injury\"] = attacks_clean[\"Injury\"].apply(move_lacerations_to_front)\n",
        "\n",
        "# Sorting injured/injury/etc.\n",
        "\n",
        "# Replace \"injuries\" with \"injury\"\n",
        "attacks_clean[\"Injury\"] = attacks_clean[\"Injury\"].str.replace(r\"\\binjuries\\b\", \"injury\", regex=True)\n",
        "\n",
        "# Replace \"injured\" with \"injury to\"\n",
        "attacks_clean[\"Injury\"] = attacks_clean[\"Injury\"].str.replace(r\"\\binjured\\b\", \"injury to\", regex=True)\n",
        "\n",
        "# If \"injury to\" is present: place it at the front of the string\n",
        "def move_injury_to_front(injury):\n",
        "    if injury is None:\n",
        "        return injury\n",
        "    else: \n",
        "      if \"injury to\" in injury:\n",
        "         return \"injury to \" + injury.replace(\"injury to\", \"\").strip()\n",
        "      else:\n",
        "         return injury.strip()\n",
        "\n",
        "# Apply the function to the \"Injury_2\" column\n",
        "attacks_clean[\"Injury\"] = attacks_clean[\"Injury\"].apply(move_injury_to_front)\n",
        "\n",
        "# Remove double whitespaces\n",
        "attacks_clean[\"Injury\"] = attacks_clean[\"Injury\"].str.replace(r\"\\s{2,}\", \" \", regex=True)\n",
        "\n",
        "# Checking how is looks now: \n",
        "# attacks_clean[\"Injury\"].value_counts().head(20)"
      ],
      "metadata": {
        "id": "O1z7Ltmof2C7"
      },
      "execution_count": 623,
      "outputs": []
    }
  ]
}